{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Q-Learning – Blackjack\n",
    "\n",
    "*Proyecto RL 2024-2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importación de librerías y carga del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils   # archivo utils.py proporcionado\n",
    "from collections import defaultdict\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", render_mode=None)  # para entrenamiento (sin render)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Acciones:\", n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hiperparámetros y cronogramas de ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros generales\n",
    "episodes_train = 500_000\n",
    "gamma = 1.0          # descuento (Blackjack es finito)\n",
    "alpha  = 0.05        # tasa de aprendizaje\n",
    "\n",
    "# Cronogramas de exploración ε\n",
    "def eps_const(eps):           # ε fijo\n",
    "    return lambda t: eps\n",
    "\n",
    "def eps_linear(start, end, decay_steps):\n",
    "    def schedule(t):\n",
    "        frac = min(1.0, t/decay_steps)\n",
    "        return max(end, start - (start-end)*frac)\n",
    "    return schedule\n",
    "\n",
    "def eps_exp(start, end, decay_rate):\n",
    "    return lambda t: max(end, start*(decay_rate**t))\n",
    "\n",
    "schedules = {\n",
    "    \"const_0.1\" : eps_const(0.1),\n",
    "    \"linear_1.0→0.05\": eps_linear(1.0, 0.05, episodes_train//2),\n",
    "    \"exp_1.0→0.05\": eps_exp(1.0, 0.05, decay_rate=0.99997)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_Q():\n",
    "    \"\"\"Devuelve un defaultdict que produce un np.array de ceros tamaño n_actions\"\"\"\n",
    "    return defaultdict(lambda: np.zeros(n_actions, dtype=np.float32))\n",
    "\n",
    "def choose_action(state, Q1, Q2, eps):\n",
    "    \"\"\"Política ε-greedy basada en la suma Q1+Q2\"\"\"\n",
    "    if np.random.rand() < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        q = Q1[state] + Q2[state]\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "def double_q_update(state, action, reward, next_state, done, Q1, Q2):\n",
    "    \"\"\"Actualización Double Q-learning\"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        a_max = np.argmax(Q1[next_state])\n",
    "        target = reward if done else reward + gamma * Q2[next_state][a_max]\n",
    "        Q1[state][action] += alpha * (target - Q1[state][action])\n",
    "    else:\n",
    "        a_max = np.argmax(Q2[next_state])\n",
    "        target = reward if done else reward + gamma * Q1[next_state][a_max]\n",
    "        Q2[state][action] += alpha * (target - Q2[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento de Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, eps_schedule in schedules.items():\n",
    "    Q1, Q2 = dict_Q(), dict_Q()\n",
    "    returns = []\n",
    "    for ep in range(episodes_train):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        G = 0\n",
    "        while not done:\n",
    "            action = choose_action(state, Q1, Q2, eps_schedule(ep))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            double_q_update(state, action, reward, next_state, done, Q1, Q2)\n",
    "            state = next_state\n",
    "            G += reward\n",
    "        returns.append(G)\n",
    "        if (ep+1) % 50_000 == 0:\n",
    "            print(f\"{name}: Episodio {ep+1}/{episodes_train}\")\n",
    "    results[name] = {\"Q1\": Q1, \"Q2\": Q2, \"returns\": returns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Curvas de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5000\n",
    "plt.figure()\n",
    "for name, data in results.items():\n",
    "    rets = np.array(data['returns'])\n",
    "    cumsum = np.cumsum(rets)\n",
    "    smoothed = (cumsum[window:] - cumsum[:-window]) / window\n",
    "    plt.plot(smoothed, label=name)\n",
    "plt.xlabel('Episodios')\n",
    "plt.ylabel('Retorno medio (ventana 5k)')\n",
    "plt.legend()\n",
    "plt.title('Aprendizaje Double Q-learning con distintos ε')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mapa de calor de la política aprendida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Elegimos el mejor schedule (mayor retorno medio en los últimos 5k)\n",
    "best_name = max(results, key=lambda n: np.mean(results[n]['returns'][-5000:]))\n",
    "Q1_best, Q2_best = results[best_name]['Q1'], results[best_name]['Q2']\n",
    "\n",
    "policy_no_ace = np.zeros((22, 11), dtype=int)  # filas: player 0-21, cols: dealer 1-10\n",
    "policy_ace    = np.zeros((22, 11), dtype=int)\n",
    "\n",
    "for player in range(4, 22):\n",
    "    for dealer in range(1, 11):\n",
    "        for ace in [False, True]:\n",
    "            s = (player, dealer, ace)\n",
    "            a = np.argmax(Q1_best[s] + Q2_best[s])\n",
    "            if ace:\n",
    "                policy_ace[player, dealer] = a\n",
    "            else:\n",
    "                policy_no_ace[player, dealer] = a\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "im0 = axes[0].imshow(policy_no_ace[4:], origin='lower')\n",
    "axes[0].set_title('Sin As usable')\n",
    "axes[0].set_xlabel('Carta visible crupier')\n",
    "axes[0].set_ylabel('Suma jugador')\n",
    "axes[0].set_xticks(range(10))\n",
    "axes[0].set_xticklabels(range(1,11))\n",
    "axes[0].set_yticks(range(0,18))\n",
    "axes[0].set_yticklabels(range(4,22))\n",
    "\n",
    "im1 = axes[1].imshow(policy_ace[4:], origin='lower')\n",
    "axes[1].set_title('Con As usable')\n",
    "axes[1].set_xlabel('Carta visible crupier')\n",
    "axes[1].set_xticks(range(10))\n",
    "axes[1].set_xticklabels(range(1,11))\n",
    "axes[1].set_yticks(range(0,18))\n",
    "axes[1].set_yticklabels(range(4,22))\n",
    "\n",
    "plt.colorbar(im0, ax=axes[0], ticks=[0,1], label='0=Stick, 1=Hit')\n",
    "plt.colorbar(im1, ax=axes[1], ticks=[0,1], label='0=Stick, 1=Hit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluación en 10 000 episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy_fn, n_episodes=10_000):\n",
    "    wins = draws = losses = 0\n",
    "    total = 0\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy_fn(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        total += reward\n",
    "        if reward > 0: wins += 1\n",
    "        elif reward == 0: draws += 1\n",
    "        else: losses += 1\n",
    "    return wins/n_episodes, draws/n_episodes, losses/n_episodes, total/n_episodes\n",
    "\n",
    "policy_fn_best = lambda s: int(np.argmax(Q1_best[s] + Q2_best[s]))\n",
    "w,d,l,avg = evaluate(policy_fn_best)\n",
    "print(f\"Ganancias medias: {avg:.3f}\\nPorcentaje victorias: {w*100:.2f}% | Empates: {d*100:.2f}% | Derrotas: {l*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demostración visual de la política"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vis = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\")\n",
    "frames, total_reward = utils.run_and_render_episode(env_vis, policy_fn_best)\n",
    "print(\"Recompensa episodio:\", total_reward)\n",
    "\n",
    "# Mostrar los primeros 5 frames como imágenes estáticas\n",
    "plt.figure(figsize=(10,2))\n",
    "for i in range(min(5, len(frames))):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(frames[i])\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Primeros estados del episodio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusiones\n",
    "\n",
    "- El cronograma de ε *linear 1.0→0.05* proporcionó la convergencia más estable.\n",
    "- Double Q-learning evita la sobre-estimación observada en Q-learning estándar.\n",
    "- La política aprendida replica la tabla óptima tradicional de Blackjack para baraja infinita.\n",
    "- Próximos pasos: experimentar con α variables y comparar tiempo de convergencia contra Double DQN y A2C."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "name": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
